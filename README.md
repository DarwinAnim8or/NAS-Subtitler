# NAS-Subtitler


<div align="center">

*ğŸš€ A dedicated subtitle tool for your NAS media library.*

**Languages:** [English](README.md) | [ä¸­æ–‡](README_CN.md) | [ç¹é«”ä¸­æ–‡](README_TW.md) | [Deutsch](README_DE.md) | [EspaÃ±ol](README_ES.md) | [FranÃ§ais](README_FR.md) | [æ—¥æœ¬èª](README_JA.md) | [í•œêµ­ì–´](README_KO.md) | [Ğ ÑƒÑÑĞºĞ¸Ğ¹](README_RU.md)

</div>

**Put subtitle work back on your NASï½œMake your NAS talk.**

On a quiet weekend night, you just want to press play. NAS-Subtitler watches your mapped **`/data`** folder: detects new videos â†’ slices clean audio locally â†’ auto transcription with **context-aware translation** â†’ writes back neat subtitles in the original directory. No progress-bar babysitting, no media leaving your server.
This â€œof course it should work like thisâ€ experience comes from the self-hosting ethos â€” **local control, privacy first. Your media, your server, your way.**

## âœ… Who It's For (Typical Scenarios)

**NAS (Jellyfin / Plex / Emby)**  

- Focus: library-wide automation, sane naming, plug-and-play  
- Expectation: map directory â†’ queue â†’ write back same-named SRT, without disturbing the media order

**Privacy- and compliance-minded organizations (legal/medical training, etc.)**  

- Focus: local computation, auditability  
- Expectation: only send the **minimal audio segments** to your chosen compatible endpoint when necessary

**Open-source users and extenders**  

- Focus: replaceable, extensible  
- Expectation: Docker-first, OpenAI-compatible APIs, clear roadmap, PR-friendly

---

## âœ¨ Why NAS-Subtitler (In Plain English)

- **Local-first & privacy-friendly**  
Use **Silero-VAD (ONNX)** on your NAS to detect â€œwhere speech happens,â€ slice audio cleanly and lightly, then transcribe and translate â€” the **full video never leaves** your server.
- **Context-aware translation experience**  [Planned]
Not a choppy â€œsentence-by-sentenceâ€ machine translation. We **merge by semantic windows â†’ translate â†’ fill back**, making the subtitle read as if a human wrote it â€” suitable for movies and online courses alike.
- **Library-level automation, reliable like an appliance**  
Watch directories â†’ schedule queues â†’ retry on failures â†’ **write back same-named SRT**, naturally compatible with Jellyfin/Plex/Embyâ€™s recognition rules.
- **Multilingual & bilingual output**  [Planned]
Generate multiple languages at once; optional **Chineseâ€“English side-by-side** (same file or separate files), good for both learning and viewing.
- **Open-source & Docker-first**  
Transparent and auditable; spin up with one command; clear dependencies and update cadence. FFmpeg handles A/V extraction and processing â€” mature ecosystem, stable across platforms.

---

## ğŸ§© How It Works

```
/data (videos)
   â””â”€ Watch  â†’  FFmpeg extract audio  â†’  Local Silero-VAD (ONNX) slicing
                   â””â†’ Segment recognition (ASR) â†’ whisper-v3 transcription â†’ punctuation/timeline fixes [Planned]
                   â””â†’ Write subtitles (SRT / VTT / ASS, as needed)
Web UI: start tasks / track progress / one-click cancel         DB: persist tasks and configuration
```

- **FFmpeg**: audio extraction / format processing (industry-standard)
- **Silero-VAD (ONNX)**: fast, lightweight, accurate voice activity detection â€” ideal for NAS
- **OpenAI-compatible protocol**: bring your own API key or endpoint; control cost and speed

---

## ğŸš€ Quick Start (Docker Compose)

1. **Prerequisite**: NAS supports Docker  
2. **Clone or pull image**: `git clone ...` (or pull the image directly)  
3. **Start**:

```yaml
services:
  nas-subtitler:
    image: ghcr.io/yourname/nas-subtitler:latest
    container_name: nas-subtitler
    environment:
      - TZ=Asia/Shanghai
      - MOUNT_DIR=/data
    volumes:
      - /path/to/videos:/data
      - /path/to/config:/app/data/config
    ports:
      - "3000:3000"
    restart: unless-stopped
```

5. **Access**: open `http://<NAS-IP>:3000` to enter the Web UI
6. **Setup**: go to Settings and enter your OpenAI API key

---

## ğŸ–±ï¸ Usage (5-Minute Onboarding)

1. **Choose directory**: point to your video folder (e.g., `/data/Movies`)  
2. **Start a task**: click â€œStart Processing,â€ choose transcription/translation target language, bilingual output, etc.  
3. **Monitor progress**: use the â€œTasksâ€ page to view queue and status; supports one-click cancel/retry  
4. **Output**: subtitles are generated in the **same folder as the video** (e.g., `movie.srt` or `movie.zh-en.srt`)  
5. **Media server recognition**: rescan your library to see subtitles take effect

---

## ğŸ§° Feature Overview

- **Truly open-source**: fully transparent; runs locally on NAS to ensure data privacy  
- **Self-serve (BYO-Key)**: use your own OpenAI API key for transcription and translation â€” **full control**  
- **Context-aware translation [Planned]**: process by semantic windows; coherent, natural translations without abruptness  
- **Bilingual support**: optional Chineseâ€“English side-by-side (same or separate files)  
- **Batch processing**: one click to auto-generate subtitles for movies/TV series  
- **Multilingual support**: improved multi-language recognition and **punctuation fixes** for smoother reading

---

## ğŸ§± Dependencies (For Engineers)

- **FFmpeg**: audio/video extraction and processing  
- **Silero-VAD (ONNX)**: voice activity detection (VAD)  
- **Node.js and related packages**: e.g., Prisma (database), EJS (templates), etc.  
- **OpenAI API (self-serve)**: any endpoint conforming to the compatible protocol can be switched

---

## ğŸ—’ï¸ Changelog

- **v1.0.0**  
  - Added **concurrent control for batch tasks**, faster and more stable processing  
  - Fixed various UI issues and edge-case errors

> For full changes, click â€œView Changelogâ€ in the Web UI.

---

## ğŸ§­ Roadmap

- [ ] **Translation** â†’ end-to-end transcription + LLM translation, ensure usefulness
- [ ] **Translation second pass** â†’ use LLM to refine already translated subtitles
- [ ] **Timeline optimization** â†’ improve subtitle timing alignment
- [ ] **Lightweight concurrent recognition** â†’ rate-limited parallelism to speed up long videos  
- [ ] **Web preview** â†’ quick verification via timeline/waveform  
- [ ] **More providers** â†’ support more vendors

---

## ğŸ¤ Contributing & Community

Contributions welcome! Please fork the repo and submit a Pull Request.  
For larger changes, consider opening an Issue first; contributions to platform templates (Synology/Unraid/TrueNAS), provider adapters, and i18n are welcome.

---

## ğŸ“ License

This project is licensed under **MIT** (see the repository for details).

---

## ğŸ™ Thanks & References

- FFmpeg â€” the technical foundation
- Silero-VAD (ONNX)